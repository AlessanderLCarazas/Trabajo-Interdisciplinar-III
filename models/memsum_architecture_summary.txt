MemSum Architecture Summary
===========================

MemSum(
  (sentence_encoder): SentenceEncoder(
    (word_embedding): Embedding(5000, 200, padding_idx=0)
    (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (document_encoder): DocumentEncoder(
    (projection): Linear(in_features=256, out_features=256, bias=True)
    (pos_encoding): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (memory_module): MemoryModule(
    (memory_update): GRU(256, 256, batch_first=True, bidirectional=True)
    (memory_projection): Linear(in_features=512, out_features=256, bias=True)
    (memory_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (gate): Sequential(
      (0): Linear(in_features=512, out_features=256, bias=True)
      (1): Sigmoid()
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (extraction_policy): ExtractionPolicy(
    (position_embedding): Embedding(1000, 64)
    (feature_fusion): Sequential(
      (0): Linear(in_features=576, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): GELU(approximate='none')
      (3): Dropout(p=0.1, inplace=False)
    )
    (policy_head): Sequential(
      (0): Linear(in_features=256, out_features=128, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=128, out_features=1, bias=True)
    )
    (value_head): Sequential(
      (0): Linear(in_features=256, out_features=128, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=128, out_features=1, bias=True)
    )
  )
)

Total parameters: 5,502,530
Trainable parameters: 5,502,530
