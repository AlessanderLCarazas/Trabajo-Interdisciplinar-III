# MemSum para BookSum Dataset

ImplementaciÃ³n de **MemSum** (Multi-step Episodic Markov decision process extractive SUMmarizer) adaptado para el dataset **BookSum** con soporte completo para GPU RTX 3050.

## ğŸš€ DescripciÃ³n

Este proyecto implementa MemSum, un modelo de **resumen extractivo** que utiliza **aprendizaje por refuerzo** para seleccionar las oraciones mÃ¡s importantes de documentos largos. La implementaciÃ³n estÃ¡ optimizada para:

- ğŸ“š **Dataset BookSum**: ResÃºmenes de capÃ­tulos de libros
- ğŸ® **GPU RTX 3050**: Optimizado para 4GB de VRAM
- ğŸ§  **Memoria EpisÃ³dica**: Evita redundancia recordando extracciones previas
- ğŸ¯ **Aprendizaje por Refuerzo**: PolÃ­tica entrenada con recompensas ROUGE

## ğŸ—ï¸ Arquitectura

### Componentes Principales:

1. **Sentence Encoder**: BiLSTM para codificar oraciones
2. **Document Encoder**: Transformer para contexto global
3. **Memory Module**: LSTM + Attention para historia de extracciones
4. **Extraction Policy**: Red neuronal para decisiones de extracciÃ³n

### Flujo de Datos:
```
Texto â†’ Oraciones â†’ Sentence Encoder â†’ Document Encoder
                                           â†“
PolÃ­tica â† Memory Module â† Historia de Extracciones
```

## ğŸ› ï¸ InstalaciÃ³n

### Prerrequisitos
- Python 3.8+
- CUDA 13.0 (para GPU RTX 3050)
- 8GB+ RAM
- 20GB+ espacio libre

### 1. Clonar y configurar entorno
```bash
cd /home/lagusa/Documentos/TI3
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o .venv\Scripts\activate  # Windows
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 3. Verificar GPU
```bash
python -c "import torch; print(f'GPU disponible: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No disponible\"}')"
```

## ğŸ“Š ConfiguraciÃ³n

### Archivo de configuraciÃ³n principal: `configs/booksum_config.yaml`

ConfiguraciÃ³n optimizada para RTX 3050:
- **Batch size**: 4 (para 4GB VRAM)
- **Accumulation steps**: 8 (batch efectivo de 32)
- **Mixed precision**: Habilitado
- **Gradient checkpointing**: Para reducir memoria

## ğŸš‚ Entrenamiento

### Entrenamiento bÃ¡sico
```bash
python train.py --config configs/booksum_config.yaml
```

### Entrenamiento con opciones avanzadas
```bash
python train.py \
    --config configs/booksum_config.yaml \
    --epochs 20 \
    --lr 1e-4 \
    --batch_size 4 \
    --wandb \
    --seed 42
```

### Reanudar entrenamiento
```bash
python train.py \
    --config configs/booksum_config.yaml \
    --resume checkpoints/checkpoint_epoch_10.pt
```

### Monitoreo durante entrenamiento
```bash
# En otra terminal
tail -f training.log

# O si usas wandb
# Ve a https://wandb.ai/tu-usuario/memsum-booksum
```

## ğŸ§ª EvaluaciÃ³n

### Evaluar modelo entrenado
```bash
python evaluate.py checkpoints/best_model.pt \
    --config configs/booksum_config.yaml \
    --split test \
    --output results.json
```

### Generar resumen de texto especÃ­fico
```bash
python evaluate.py checkpoints/best_model.pt \
    --text "Tu texto aquÃ­ para resumir..."
```

## ğŸ“ Estructura del Proyecto

```
TI3/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # ConfiguraciÃ³n del modelo
â”‚   â”œâ”€â”€ data_loader.py      # Carga y procesamiento de BookSum
â”‚   â”œâ”€â”€ model.py           # Arquitectura MemSum
â”‚   â””â”€â”€ trainer.py         # Entrenamiento con RL
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ booksum_config.yaml # ConfiguraciÃ³n principal
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ (scripts auxiliares)
â”œâ”€â”€ data/                  # Datos y vocabulario
â”œâ”€â”€ models/               # Modelos guardados
â”œâ”€â”€ checkpoints/          # Checkpoints de entrenamiento
â”œâ”€â”€ logs/                # Logs y mÃ©tricas
â”œâ”€â”€ train.py             # Script de entrenamiento
â”œâ”€â”€ evaluate.py          # Script de evaluaciÃ³n
â”œâ”€â”€ requirements.txt     # Dependencias
â””â”€â”€ README.md           # Este archivo
```

## ğŸ¯ Resultados Esperados

### MÃ©tricas ROUGE esperadas en BookSum:
- **ROUGE-1**: ~0.42-0.48
- **ROUGE-2**: ~0.18-0.24  
- **ROUGE-L**: ~0.38-0.45

### Tiempo de entrenamiento (RTX 3050):
- **Por Ã©poca**: ~45-60 minutos
- **Entrenamiento completo**: ~12-15 horas

## ğŸ”§ Optimizaciones para RTX 3050

### Configuraciones especÃ­ficas:
```yaml
training:
  batch_size: 4              # Optimizado para 4GB VRAM
  accumulation_steps: 8      # Batch efectivo de 32
  
model:
  hidden_dim: 256           # Balanceado rendimiento/memoria
  max_doc_len: 500         # Longitud mÃ¡xima de documento
  
device:
  mixed_precision: true     # Reduce uso de memoria 50%
  use_gpu: true
```

### Tips de optimizaciÃ³n:
1. **Monitoring memoria**: `nvidia-smi` cada 30s
2. **Batch dinÃ¡mico**: Reduce batch_size si hay OOM
3. **Gradient checkpointing**: Habilitado automÃ¡ticamente
4. **LiberaciÃ³n cachÃ©**: `torch.cuda.empty_cache()` automÃ¡tico

## ğŸ› Troubleshooting

### Error: CUDA Out of Memory
```bash
# SoluciÃ³n 1: Reducir batch size
python train.py --batch_size 2

# SoluciÃ³n 2: Sin mixed precision
# Editar config: mixed_precision: false

# SoluciÃ³n 3: Reducir dimensiones del modelo
# Editar config: hidden_dim: 128
```

### Error: Dataset no encontrado
```bash
# El cÃ³digo crearÃ¡ datos dummy automÃ¡ticamente
# O descargar BookSum manualmente:
huggingface-cli login
python -c "from datasets import load_dataset; load_dataset('kmfoda/booksum')"
```

### Entrenamiento muy lento
```bash
# Verificar GPU
nvidia-smi

# Reducir num_workers si hay problemas I/O
python train.py --config configs/booksum_config.yaml
# Editar config: num_workers: 0
```

## ğŸ“š Referencias

- **Paper Original**: [MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes](https://aclanthology.org/2022.acl-long.450/)
- **BookSum Dataset**: [BookSum: A Collection of Datasets for Long-form Narrative Summarization](https://arxiv.org/abs/2105.08209)
- **Repositorio Original**: [nianlonggu/MemSum](https://github.com/nianlonggu/MemSum)

## ğŸ¤ ContribuciÃ³n

Para contribuir al proyecto:
1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -am 'AÃ±adir nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Crea un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para detalles.

## ğŸ™ Agradecimientos

- Equipo original de MemSum (Gu et al., 2022)
- Dataset BookSum (KryÅ›ciÅ„ski et al., 2021)  
- Comunidad de Transformers de Hugging Face
- PyTorch y NVIDIA por el soporte GPU

---

## ğŸš€ Quick Start

```bash
# 1. Activar entorno
source .venv/bin/activate

# 2. Entrenar modelo
python train.py --config configs/booksum_config.yaml --epochs 5

# 3. Evaluar
python evaluate.py checkpoints/best_model.pt

# 4. Generar resumen
python evaluate.py checkpoints/best_model.pt \
    --text "Tu texto largo aquÃ­..."
```

**Â¡Listo para entrenar MemSum en BookSum con tu RTX 3050! ğŸš€**